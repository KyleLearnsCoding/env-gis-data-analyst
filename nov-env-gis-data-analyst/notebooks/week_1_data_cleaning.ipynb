{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0526c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.26200-SP0\n"
     ]
    }
   ],
   "source": [
    "#Environment check /installs\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6404cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports & paths (annotated)\n",
    "import pandas as pd             # Core data handling: read, clean, and transform CSVs\n",
    "import numpy as np              # Numerical operations, NaN handling, and unit conversions\n",
    "import re                       # Regular expressions for parsing messy strings (coords, units)\n",
    "from pathlib import Path         # Cross-platform file path handling for locating input/output files\n",
    "from datetime import datetime    # Working with timestamps and generating sample date ranges\n",
    "from dateutil import parser as dateparser  # Flexible parsing of inconsistent date formats\n",
    "\n",
    "RAW_DIR = Path(\"../data\")   #location of csvs\n",
    "PATH_STATIONS = RAW_DIR / \"raw_station_metadata.csv\"\n",
    "PATH_SAMPLES = RAW_DIR / \"raw_water_samples.csv\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1385f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===raw_station_metadata.csv ===\n",
      "\n",
      "--- FIRST FEW LINES ---\n",
      "THIS IS A HEADER,foo, Station ID ,SiteName,Latitude,Longitude,Coordinates,Elev,Agency_Code,CRS,Notes\n",
      "StationMetadata v0.1,bar,,,,,,,,,\n",
      ",,S001,River Site 1,37.872701,-78.532764,,246.9 m,City Lab,,\n",
      ",,S002,River Site 2,40.753572,-77.566062,,76.5 m,EPA,,Near bridge\n",
      ",,S003,River Site 3,,,\"39.65997,-75.801949\",35.4 m,USGS,EPSG:4326,Upstream\n",
      "\n",
      "--- LAST FEW LINES ---\n",
      ",,S012,River Site 12,40.849549,-78.40261,,203.1 m,EPA,,Downstream\n",
      ",,S013,River Site 13  ,\"40°9'43\"\"N\",\"75°53'10\"\"S\",,23.4 m,City Lab,EPSG:4269,\n",
      ",,S014,River Site 14,\"37°3'42\"\"N\",\"75°15'38\"\"S\",,364.6 m,DWR,WGS84,\n",
      ",,S015,River Site 15,36.909125,-79.628397,,110.9 m,USGS,EPSG:4326,Near bridge\n",
      ",,S004,River Site 4,\"38°59'35\"\"N\",\"76°32'39\"\"S\",,380.1 m,USGS,,Downstream\n",
      "\n",
      "===raw_water_samples.csv ===\n",
      "\n",
      "--- FIRST FEW LINES ---\n",
      "Sample ID,StationCode,Sample Date,Analyte,Result,Units,Method ID,DetectLimit,Temp,Remarks,footer\n",
      "WQ1000,S014,2025-10-20,Chloride,2698.40,ug/L,SM 4500-NO3,0.07,66.7 F,,\n",
      "WQ1001,S009,25-Oct-2025,AMMONIUM,ND,mg/L,SM 4500-NH3,0.11,60.8 F, ,\n",
      "WQ1002,S006,2025-10-17,Ammonium,<0.68,mg/L,ISO 7027,0.19,61.4 F,,\n",
      "WQ1003, S011,2025-10-16,Nitrate as N,ND,ug/L,SM 4500-NO3,0.06,16.7 C,,\n",
      "\n",
      "--- LAST FEW LINES ---\n",
      "WQ1116,S003,10/28/2025,Turbidity,2324.52,ug/L,SM 4500-NO3,0.19,65.9 F,matrix spike,\n",
      "WQ1117,S001,2025-10-11,Turbidity,ND,ug/L,EPA 300.0,0.17,18.4 C, ,\n",
      "WQ1118,S013,2025-10-17,Chloride,ND,ug/L,ISO 7027,0.05,12.9 C,,\n",
      "WQ1119,S007,10/29/2025,Ammonium,<0.62,mg/L,ISO 7027,0.18,12.3 C,duplicate sample,\n",
      ",,,,,,,,,,End of File\n"
     ]
    }
   ],
   "source": [
    "#Inspect Raw Structure (diagnose junk headers, footers, columns)\n",
    "\n",
    "#Read just a small sample from the start and end of each CSV to preview structure\n",
    "def quick_peek_csv(path, n=5): #5 get used by default unless specified, ie. quick_peek_csv(PATH_STATIONS, n=10)\n",
    "    \"\"\"Peek at the first and last few lines of a CSV file to detect header/footer artifacts.\"\"\"\n",
    "    print(f\"\\n==={path.name} ===\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:   #open file located at path, in read mode (\"r\"), utf-8 ensures special characters read correctly, skips errors instead of failing to execute code\n",
    "        lines = f.readlines()   #reads all lines\n",
    "    print(\"\\n--- FIRST FEW LINES ---\")\n",
    "    for l in lines[:n]: #limits line read to first 5 (or specified)\n",
    "        print(l.strip())\n",
    "    print(\"\\n--- LAST FEW LINES ---\")\n",
    "    for l in lines[-n:]:    #limits line read to last 5 (or specified) note colon placement different\n",
    "        print(l.strip())\n",
    "\n",
    "quick_peek_csv(PATH_STATIONS)\n",
    "quick_peek_csv(PATH_SAMPLES)\n",
    "\n",
    "#PICK UP HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29739b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Column names: raw_station_metadata.csv ---\n",
      "['THIS IS A HEADER', 'foo', ' Station ID ', 'SiteName', 'Latitude', 'Longitude', 'Coordinates', 'Elev', 'Agency_Code', 'CRS', 'Notes']\n",
      "\n",
      "--- Column names: raw_water_samples.csv ---\n",
      "['Sample ID', 'StationCode', 'Sample Date', 'Analyte', 'Result', 'Units', 'Method ID', 'DetectLimit', 'Temp', 'Remarks', 'footer']\n"
     ]
    }
   ],
   "source": [
    "# Load minimally to inspect column names (no cleaning yet)\n",
    "stations_probe = pd.read_csv(PATH_STATIONS, nrows=10, dtype=str)\n",
    "samples_probe  = pd.read_csv(PATH_SAMPLES,  nrows=10, dtype=str)\n",
    "\n",
    "print(\"\\n--- Column names: raw_station_metadata.csv ---\")\n",
    "print(stations_probe.columns.tolist())\n",
    "\n",
    "print(\"\\n--- Column names: raw_water_samples.csv ---\")\n",
    "print(samples_probe.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d962dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  THIS IS A HEADER  foo  Station ID       SiteName   Latitude   Longitude          Coordinates     Elev Agency_Code        CRS  \\\n",
       " 1              NaN  NaN         S001  River Site 1  37.872701  -78.532764                  NaN  246.9 m    City Lab        NaN   \n",
       " 2              NaN  NaN         S002  River Site 2  40.753572  -77.566062                  NaN   76.5 m         EPA        NaN   \n",
       " 3              NaN  NaN         S003  River Site 3        NaN         NaN  39.65997,-75.801949   35.4 m        USGS  EPSG:4326   \n",
       " \n",
       "          Notes  \n",
       " 1               \n",
       " 2  Near bridge  \n",
       " 3     Upstream  ,\n",
       "   Sample ID StationCode  Sample Date   Analyte   Result Units    Method ID DetectLimit    Temp Remarks footer\n",
       " 0    WQ1000        S014   2025-10-20  Chloride  2698.40  ug/L  SM 4500-NO3        0.07  66.7 F     NaN    NaN\n",
       " 1    WQ1001        S009  25-Oct-2025  AMMONIUM       ND  mg/L  SM 4500-NH3        0.11  60.8 F     NaN    NaN\n",
       " 2    WQ1002        S006   2025-10-17  Ammonium    <0.68  mg/L     ISO 7027        0.19  61.4 F     NaN    NaN)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Robust Load (handle junk header/footer rows, everything as string)\n",
    "\n",
    "def read_raw(path):\n",
    "    df = pd.read_csv(path, dtype=str, keep_default_na=False, na_values=[\"\", \" \"])   #dont translate the default conditions as NA (b/c some might be legitamit data), just \"\" and \" \"\n",
    "    #Drop obvious junk rows\n",
    "    #df = df[[c for c in df.columns if not c.startswith(\"Unnamed\")]]  **left this out b/c practice dataset doesnt include \"Unnamed\" in any cell\n",
    "    df = df[~df.apply(lambda r: r.astype(str).str.contains(\"End of File|StationMetadata v0.1\", case=False).any(), axis=1)]  #.apply(..axis=1) means apply function to every row, lambda r is an anonymous function operating on each row \"r\", case=False means ignore casing, .any() returns True if any column in row matched pattern\n",
    "    return df\n",
    "\n",
    "raw_stations = read_raw(PATH_STATIONS)\n",
    "raw_samples = read_raw(PATH_SAMPLES)\n",
    "\n",
    "raw_stations.head(3), raw_samples.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8014d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['this_is_a_header', 'foo', 'station_id', 'sitename', 'latitude', 'longitude', 'coordinates', 'elev', 'agency_code', 'crs', 'notes'], dtype='object'),\n",
       " Index(['sample_id', 'stationcode', 'sample_date', 'analyte', 'result', 'units', 'method_id', 'detectlimit', 'temp', 'remarks', 'footer'], dtype='object'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Standardize column names (snake_case, trim)\n",
    "\n",
    "#Column name normalization\n",
    "def to_snake(s):\n",
    "    #regular expression (regex) string cleaning.  s is variable holding any string\n",
    "    s = re.sub(r\"\\s+\", \"_\", s.strip())  #re=regular expression function, sub() substitute, r\"\\s+\" \\s any whitespace char, + one or more in a row, repalce with _\n",
    "    s=re.sub(r\"[^\\w]+\", \"_\", s) #\\w any word char, ^ not: [^\\w]+ one or more non-word char's(not letter digit or underscore): includes punctuation, symbols, special chars, replace with _\n",
    "    return s.lower().strip(\"_\")\n",
    "\n",
    "raw_stations.columns = [to_snake(c) for c in raw_stations.columns]\n",
    "raw_samples.columns = [to_snake(c) for c in raw_samples.columns]\n",
    "\n",
    "#Trim all string cells\n",
    "def trim_df(df):\n",
    "    return df.map(lambda x: x.strip() if isinstance(x, str) else x) #older ver. pandas is applymap()\n",
    "\n",
    "raw_stations = trim_df(raw_stations)\n",
    "raw_samples = trim_df(raw_samples)\n",
    "\n",
    "raw_stations.columns, raw_samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a74051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and Standardize STATIONS: LATITUDE AND LONGITUDE\n",
    "\n",
    "#Stations:pick canonical Keys/columns\n",
    "stations = raw_stations.copy()\n",
    "\n",
    "#Normalize whitespace and remove accidental trailing spaces in IDs\n",
    "stations[\"station_id\"] = stations[\"station_id\"].str.strip()\n",
    "\n",
    "#If coordinates sometimes appear as \"lat,lon\" in single field, split them (we know \"coordinates\" column exists, see above printouts)\n",
    "if \"coordinates\" in stations.columns:\n",
    "    coords_split = stations[\"coordinates\"].str.split(\",\", n=1, expand=True)\n",
    "    if isinstance(coords_split, pd.DataFrame) and coords_split.shape[1] == 2:   #code only proceeds if the above resulted in a DataFrame(structural check) and a pair of data, rather than a single or no data in a row coords_split.shape[1] == 2\n",
    "        stations[\"lat_from_coords\"] = coords_split[0].str.strip()\n",
    "        stations[\"lon_from_coords\"] = coords_split[1].str.strip()\n",
    "\n",
    "#Prefer explicity latitude/longitude columns if they look valid, else fallback to ..._from_coords\n",
    "lat_col = \"latitude\" if \"latitude\" in stations.columns else \"lat_from_coords\"\n",
    "lon_col = \"longitude\" if \"longitude\" in stations.columns else \"lon_from_coords\"\n",
    "\n",
    "# Parse decimal or DMS strings to decimal degrees\n",
    "def parse_coord(val):\n",
    "    if pd.isna(val) or val == \"\":\n",
    "        return np.nan\n",
    "    s = str(val).strip()\n",
    "    # is value a decimal?\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # is value DMS? like 40°26'46\"N\n",
    "    m = re.match(r\"^\\s*(\\d+)[^\\d]+(\\d+)[^\\d]+(\\d+)\\s*([NSEW])\\s*$\", s) \n",
    "    #r means raw string, tells python treat \\ literally.  ^ beginning of string. $ end of string.  \\s whitespace, * zero or more: \\s* means allow some whitespace\n",
    "    #\\d any digit, + one or more digist, () capture this portion as a group ie. (\\d+).  [^\\d]+ means \"at the start\" one or more characters that are not digits(like punctuation/symbols).\n",
    "    #([NSEW]) captures one uppercase character from the set N, S, E, or W.\n",
    "    # + between bracketed groups means \"join\", its just bridge between definitions of the regular expressions\n",
    "    if m:\n",
    "        d, mi, sec, hemi = m.groups()\n",
    "        dec = float(d) + float(mi)/60 + float(sec)/3600\n",
    "        if hemi in [\"S\", \"W\"]:\n",
    "            dec = -dec\n",
    "        return dec\n",
    "    # If \"lon,lat\" sneaks in\n",
    "    if \",\" in s:\n",
    "        try:\n",
    "            a, b = s.split(\",\", 1)\n",
    "            # we won't assign here; we parse elsewhere\n",
    "            return np.nan\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "#Puts our final latitude and longitude data in columns \"lat\" and \"lon\" after all the above checks\n",
    "stations[\"lat\"] = stations.get(lat_col, \"\").apply(parse_coord)\n",
    "stations[\"lon\"] = stations.get(lon_col, \"\").apply(parse_coord)\n",
    "\n",
    "#If lat/lon were swapped in some rows (e.g., lat looks like -75) try swapping when it makes sense\n",
    "def maybe_swap(row):\n",
    "    lat, lon = row[\"lat\"], row[\"lon\"]\n",
    "    if pd.notna(lat) and pd.notna(lon):\n",
    "        #Lat should be roughly -90..90; lon -180..180\n",
    "        if (lat < -90 or lat > 90) and (-90 <= lon <= 90):  #checks if lat seem too large and lon seems small, condition to swap them\n",
    "            return pd.Series({\"lat\": lon, \"lon\": lat})\n",
    "    return pd.Series({\"lat\": lat, \"lon\": lon})  #creates since row (or column) of data as one-dimentional array\n",
    "\n",
    "stations[[\"lat\", \"lon\"]] = stations.apply(maybe_swap, axis=1) #double brackets says \"select these two columns\" in DF \"stations\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bcffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and Standardize STATIONS: ELEVATION, AGENCY CODES, MISC\n",
    "\n",
    "#Elevation: convert to meters (if ft present)\n",
    "def parse_elev(s):\n",
    "    if pd.isna(s) or s == \"\":\n",
    "        return np.nan\n",
    "    m = re.match(r\"^\\s*([-+]?\\d+(\\.\\d+)?)\\s*(ft|feet|foot|m|meters?)?\\s*$\", s, flags=re.I) #means?\n",
    "\n",
    "#PICK UP HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb38c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Optional: PostGIS connection imports (annotated)\n",
    "\n",
    "from sqlalchemy import create_engine, text  \n",
    "# → SQLAlchemy is the core interface to databases.\n",
    "#   `create_engine()` builds a connection string for PostgreSQL/PostGIS.\n",
    "#   `text()` allows you to execute raw SQL commands (e.g., creating schemas, geometry columns).\n",
    "\n",
    "import psycopg2  \n",
    "# → PostgreSQL driver used under the hood by SQLAlchemy to communicate with the PostGIS container.\n",
    "#   Required for executing SQL and writing DataFrames to PostGIS tables.\n",
    "\n",
    "# (Optional, for geospatial validation later)\n",
    "# from shapely.geometry import Point  \n",
    "# → If you later want to create geometry objects or verify coordinates before sending to PostGIS.\n",
    "\n",
    "# from geopandas import GeoDataFrame  \n",
    "# → Optional upgrade of pandas DataFrames into GeoDataFrames for spatial operations before export.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nov-env-gis)",
   "language": "python",
   "name": "nov-env-gis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
