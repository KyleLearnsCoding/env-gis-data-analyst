{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0526c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.26200-SP0\n"
     ]
    }
   ],
   "source": [
    "#Environment check /installs\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6404cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports & paths (annotated)\n",
    "import pandas as pd             # Core data handling: read, clean, and transform CSVs\n",
    "import numpy as np              # Numerical operations, NaN handling, and unit conversions\n",
    "import re                       # Regular expressions for parsing messy strings (coords, units)\n",
    "from pathlib import Path         # Cross-platform file path handling for locating input/output files\n",
    "from datetime import datetime    # Working with timestamps and generating sample date ranges\n",
    "from dateutil import parser as dateparser  # Flexible parsing of inconsistent date formats\n",
    "\n",
    "RAW_DIR = Path(\"../data\")   #location of csvs\n",
    "PATH_STATIONS = RAW_DIR / \"raw_station_metadata.csv\"\n",
    "PATH_SAMPLES = RAW_DIR / \"raw_water_samples.csv\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===raw_station_metadata.csv ===\n",
      "\n",
      "--- FIRST FEW LINES ---\n",
      "THIS IS A HEADER,foo, Station ID ,SiteName,Latitude,Longitude,Coordinates,Elev,Agency_Code,CRS,Notes\n",
      "StationMetadata v0.1,bar,,,,,,,,,\n",
      ",,S001,River Site 1,37.872701,-78.532764,,246.9 m,City Lab,,\n",
      ",,S002,River Site 2,40.753572,-77.566062,,76.5 m,EPA,,Near bridge\n",
      ",,S003,River Site 3,,,\"39.65997,-75.801949\",35.4 m,USGS,EPSG:4326,Upstream\n",
      "\n",
      "--- LAST FEW LINES ---\n",
      ",,S012,River Site 12,40.849549,-78.40261,,203.1 m,EPA,,Downstream\n",
      ",,S013,River Site 13  ,\"40°9'43\"\"N\",\"75°53'10\"\"S\",,23.4 m,City Lab,EPSG:4269,\n",
      ",,S014,River Site 14,\"37°3'42\"\"N\",\"75°15'38\"\"S\",,364.6 m,DWR,WGS84,\n",
      ",,S015,River Site 15,36.909125,-79.628397,,110.9 m,USGS,EPSG:4326,Near bridge\n",
      ",,S004,River Site 4,\"38°59'35\"\"N\",\"76°32'39\"\"S\",,380.1 m,USGS,,Downstream\n",
      "\n",
      "===raw_water_samples.csv ===\n",
      "\n",
      "--- FIRST FEW LINES ---\n",
      "Sample ID,StationCode,Sample Date,Analyte,Result,Units,Method ID,DetectLimit,Temp,Remarks,footer\n",
      "WQ1000,S014,2025-10-20,Chloride,2698.40,ug/L,SM 4500-NO3,0.07,66.7 F,,\n",
      "WQ1001,S009,25-Oct-2025,AMMONIUM,ND,mg/L,SM 4500-NH3,0.11,60.8 F, ,\n",
      "WQ1002,S006,2025-10-17,Ammonium,<0.68,mg/L,ISO 7027,0.19,61.4 F,,\n",
      "WQ1003, S011,2025-10-16,Nitrate as N,ND,ug/L,SM 4500-NO3,0.06,16.7 C,,\n",
      "\n",
      "--- LAST FEW LINES ---\n",
      "WQ1116,S003,10/28/2025,Turbidity,2324.52,ug/L,SM 4500-NO3,0.19,65.9 F,matrix spike,\n",
      "WQ1117,S001,2025-10-11,Turbidity,ND,ug/L,EPA 300.0,0.17,18.4 C, ,\n",
      "WQ1118,S013,2025-10-17,Chloride,ND,ug/L,ISO 7027,0.05,12.9 C,,\n",
      "WQ1119,S007,10/29/2025,Ammonium,<0.62,mg/L,ISO 7027,0.18,12.3 C,duplicate sample,\n",
      ",,,,,,,,,,End of File\n"
     ]
    }
   ],
   "source": [
    "#Inspect Raw Structure (diagnose junk headers, footers, columns)\n",
    "\n",
    "#Read just a small sample from the start and end of each CSV to preview structure\n",
    "def quick_peek_csv(path, n=5): #5 get used by default unless specified, ie. quick_peek_csv(PATH_STATIONS, n=10)\n",
    "    \"\"\"Peek at the first and last few lines of a CSV file to detect header/footer artifacts.\"\"\"\n",
    "    print(f\"\\n==={path.name} ===\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:   #open file located at path, in read mode (\"r\"), utf-8 ensures special characters read correctly, skips errors instead of failing to execute code\n",
    "        lines = f.readlines()   #reads all lines\n",
    "    print(\"\\n--- FIRST FEW LINES ---\")\n",
    "    for l in lines[:n]: #limits line read to first 5 (or specified)\n",
    "        print(l.strip())\n",
    "    print(\"\\n--- LAST FEW LINES ---\")\n",
    "    for l in lines[-n:]:    #limits line read to last 5 (or specified) note colon placement different\n",
    "        print(l.strip())\n",
    "\n",
    "quick_peek_csv(PATH_STATIONS)\n",
    "quick_peek_csv(PATH_SAMPLES)\n",
    "\n",
    "#PICK UP HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d962dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robust Load (handle junk header/footer rows, everything as string)\n",
    "\n",
    "def read_raw(path):\n",
    "    df = pd.read_csv(path, dtype=str, keep_default_na=False, na_values=[\"\", \" \"])   #dont translate the default conditions as NA (b/c some might be legitamit data), just \"\" and \" \"\n",
    "    #Drop obvious junk rows\n",
    "    df = df[[c for c in df.columns if not c.startswith(\"Unnamed\")]]\n",
    "    df = df[~df.apply(lambda r: r.astype(str).str.contains(\"End of File|StationMetadata v0.1\", case=False).any(), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Optional: PostGIS connection imports (annotated)\n",
    "\n",
    "from sqlalchemy import create_engine, text  \n",
    "# → SQLAlchemy is the core interface to databases.\n",
    "#   `create_engine()` builds a connection string for PostgreSQL/PostGIS.\n",
    "#   `text()` allows you to execute raw SQL commands (e.g., creating schemas, geometry columns).\n",
    "\n",
    "import psycopg2  \n",
    "# → PostgreSQL driver used under the hood by SQLAlchemy to communicate with the PostGIS container.\n",
    "#   Required for executing SQL and writing DataFrames to PostGIS tables.\n",
    "\n",
    "# (Optional, for geospatial validation later)\n",
    "# from shapely.geometry import Point  \n",
    "# → If you later want to create geometry objects or verify coordinates before sending to PostGIS.\n",
    "\n",
    "# from geopandas import GeoDataFrame  \n",
    "# → Optional upgrade of pandas DataFrames into GeoDataFrames for spatial operations before export.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nov-env-gis)",
   "language": "python",
   "name": "nov-env-gis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
